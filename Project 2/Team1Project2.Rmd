---
title: "DATS 6101 Team 1 - Project 1"
author: "Tyler Wallett, Anushka Vuppala and David Li"
# date: "today"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
    
---
```{r init, include=F}
library(tidyverse)
library(ezids)
library(class)
loadPkg("gmodels")
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret")
```
# Introduction
<span style="color: Blue;"> Our research topic aims to perform explorartory data analysis on the Nearest Earth Objects (NEO) dataset recorded by NASA. This particular dataset includes thousands of labeled observations, over the last 22 years, across multiple attributes that each aim to describe a specific object. Borrowing from the humor of the recent film *Don’t Look Up* by Adam Mckay and NASA’s success in the Double Asteroid Redirection Test (DART) mission, we thought that further describing and exploring this dataset would be opportune.</span> 

The EDA performed can help with the early detection of an asteroid. With the early detection, NASA can take quick remedies to help eliminate the threat. NASA’s DART mission (Double Asteroid Redirection Test) was done for the same purpose. It was designed to deflect an asteroid with a certain momentum by hitting it head-on or attempting to slow it down. More can be read online from this [article](https://en.wikipedia.org/wiki/Double_Asteroid_Redirection_Test).

According to NASA’s [official documentation](https://cneos.jpl.nasa.gov/about/neo_groups.html), a PHA is technically termed using parameters that measure the asteroid’s characteristics and then make a decision if the asteroid is considered hazardous based on certain thresholds. Specifically, all asteroids with an Earth Minimum Orbit intersection Distance (MOID) of 0.05 au or less and an absolute magnitude (H) of 22.0 or less are considered Potentially Hazardous Asteroids (PHAs). In other words, asteroids that can’t get any closer to the Earth (i.e., MOID) than 0.05 au (roughly 7,480,000km) or are smaller than about 140m in diameter (i.e., H=22.0) are not considered PHAs.
 
The current dataset that we have contains 90,000 rows. Each row represents a NEO identified by an ID and name. Out of these 90,000 rows, 9,000 of them are hazardous and 81,000 of them are non-hazardous. This dataset does not contain null values. Apart from the hazardous target variable, our dataset contains 5 numerical variables that can help identify if the asteroid is hazardous or not. These 5 variables are:

1.	est_diameter_min (km) – estimated minimum diameter of the NEO
1.	est_diameter_max (km) – estimated maximum diameter of the NEO
1.	relative_velocity – the velocity (in km/seconds) with which the NEO was travelling with respect to Earth 
1.	miss_distance – distance (in km) by which the asteroid missed the Earth’s surface
1.	absolute_magnitude (H) – signifies the brightness of the asteroid

Out of these 5 variables, we have 3 variables that play a primary role in determining if the asteroid was harmful or not. Those are:

1.	relative_velocity
1.	miss_distance
1.	absolute_magnitude

Throughout the rest of this RMD file, we go over each of these 3 variables in complete depth to answer our SMART question – what are some of the statistical characteristics of Nearest Earth Objects, over the last 22 years, in terms of relative velocity, miss distance and absolute magnitude, that make them hazardous or not?


```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
# knitr::opts_chunk$set(include = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# use scipen=999 to prevent scientific notation at all times
```


# Let's explore the dataset
```{r import_data, results='markup', echo=FALSE}
neo <- data.frame(read.csv("neo.csv", header = TRUE))
```
Displaying the structure of the data:
```{r, results='markup', echo=FALSE}
print(str(neo))
```
Displaying the first 6 rows of the dataset:
```{r, results='markup', echo=FALSE}
print(head(neo))
```
The number of NA values in the dataset => `r sum(is.na(neo))`


Now, we split the dataset into 2 dataframes - one for hazardous and other for non-hazardous. This is to ease out the tests performed.

```{r split dataset, results='markup', echo=FALSE}
hazardous = neo %>% filter(hazardous=="True")
non_hazardous = neo %>% filter(hazardous=="False")
```
The dimensions of hazardous asteroids dataframe: `r dim(hazardous)`

The dimensions of non_hazardous asteroids dataframe: `r dim(non_hazardous)`

# Performing measures of central tendency using Summary

Summary of all asteroids is as follows:
```{r summary, results='markup', echo=FALSE}
summary(neo)
```

# PROJECT 2

Before we begin any modeling, it is good practice to have a training and test accuracy.

# Sampling of data
## Helper functions
```{r, results='markup'}
Standardization <- function(data) {
  return (scale(data[,1:3], center = TRUE, scale = TRUE))
}

basicKNN5 <- function() {
  knn.5 <- knn(train=nasa_train, test=nasa_test, cl=nasa_train.labels, k=5)
  print(100 * sum(nasa_test.labels == knn.5)/NROW(nasa_test.labels))
  confusionMatrix(knn.5, as.factor(nasa_test.labels), mode="everything")
}
```

Below is the R code of performing the split without any sampling technique used.
We would also be using the training and testing data to check the accuracy of a base KNN model. We would implement this KNN model with the k value equal to 5 for the purpose of comparison between different sampling techniques.

## No sampling technique used:
```{r, results='markup'}
#str(neo)
set.seed(123)

neo_no_sampling = (neo %>% select(relative_velocity, absolute_magnitude, miss_distance, hazardous))
smp_size <- floor(0.75 * nrow(neo_no_sampling))
train_data_size <- sample(seq_len(nrow(neo_no_sampling)), size = smp_size)


nasa_train.labels = neo_no_sampling[train_data_size,c("hazardous")]
nasa_test.labels = neo_no_sampling[-train_data_size,c("hazardous")]

neo_no_sampling = neo_no_sampling %>% select(relative_velocity, miss_distance, absolute_magnitude)
nasa_train <- neo_no_sampling[train_data_size,]
nasa_test <- neo_no_sampling[-train_data_size,]

# Standardization
nasa_train = Standardization(nasa_train)
nasa_test = Standardization(nasa_test)

basicKNN5()

```
The above shows an accuracy of 89.5% which initially sounds great. But when we draw a confusion matrix on the above and report other evaluation metrics, we see a 30% Specificity. This is not surprising as the amount of data in minority class (hazardous) is very minimum. 

The next sampling technique we can use is called downsampling. This ensures that the data that we choose has equal proportions of hazardous and non-hazardous asteroids.
For the purpose of implementing downsampling, we will retain all the rows in hazardous and randomly sample equal number of rows from non-hazardous. In other words, we would be bringing down the number of rows in non-hazardous from ~80,000 to ~8,000 rows.

## Downsampling
```{r, results='markup'}
set.seed(123)
non_hazardous = non_hazardous[sample(nrow(non_hazardous),nrow(hazardous)),]
neo_downsampling= rbind(hazardous, non_hazardous)
nrow(neo_downsampling)

## 75% of the sample size
smp_size <- floor(0.75 * nrow(neo_downsampling))

## set the seed to make your partition reproducible
set.seed(123)
train_data_size <- sample(seq_len(nrow(neo_downsampling)), size = smp_size)

nasa_train.labels = neo_downsampling[train_data_size,c("hazardous")]
nasa_test.labels = neo_downsampling[-train_data_size,c("hazardous")]

neo_downsampling = neo_downsampling %>% select(relative_velocity, miss_distance, absolute_magnitude)
nasa_train <- neo_downsampling[train_data_size,]
nasa_test <- neo_downsampling[-train_data_size,]

# Standardization
nasa_train = Standardization(nasa_train)
nasa_test = Standardization(nasa_test)

basicKNN5()
```
From downsampling result above, we see that the results have become more realisti. In other words, we see that our Specificity rate has increased to 92% which is a great indicator of a great model. However, one biggest disadvantage of downsampling is loss of data. We are training the model with insufficient amount of data.

The next obvious sampling technique is upsampling. One such case of upsampling is SMOTE. Smote created artificial data points to bring up the data points in minority class.

## SMOTE
```{r,results='markup'}
set.seed(123)

neo_smote = (neo %>% select(relative_velocity, absolute_magnitude, miss_distance, hazardous))
neo_smote$hazardous[neo_smote$hazardous == "True"] <- 1
neo_smote$hazardous[neo_smote$hazardous == "False"] <- 0
neo_smote$hazardous = as.numeric(neo_smote$hazardous)

install.packages("smotefamily",repos='http://cran.us.r-project.org') 
library(smotefamily)
smote = SMOTE(neo_smote[1:3], neo_smote$hazardous)
newdata = smote$data
newdata = newdata %>% rename(hazardous=class)
newdata$hazardous = as.numeric(newdata$hazardous)


smp_size <- floor(0.75 * nrow(newdata))

## set the seed to make your partition reproducible
set.seed(222)
train_data_size <- sample(seq_len(nrow(newdata)), size = smp_size)


nasa_train.labels = newdata[train_data_size,c("hazardous")]
nasa_test.labels = newdata[-train_data_size,c("hazardous")]

nasa_train <- newdata[train_data_size,]
nasa_test <- newdata[-train_data_size,]

nasa_train = scale(nasa_train[,1:3], center = TRUE, scale = TRUE)
nasa_test = scale(nasa_test[,1:3], center = TRUE, scale = TRUE)

basicKNN5()
```

From the above, we can see that we have finally received a higher rate of Recall. 

# Count Plot of data points after SMOTE
```{r, results='markup'}
tab=matrix(c(sum(neo$hazardous=="True"), sum(neo$hazardous=="False"), sum(newdata$hazardous==1), sum(newdata$hazardous==0)), ncol = 2, byrow = TRUE)
colnames(tab) <- c('Hazardous','Non Hazardous')
rownames(tab) <- c('Orginial Data','Smote Data')
tab <- as.table(tab)
my_data <- as.data.frame(tab) 
ggplot(my_data,                                   
       aes(x = Var1,
           y = Freq, fill= Var2)) + 
  geom_bar(stat = "identity", position = "dodge") +
  ggtitle('Count of hazardous and non-hazardous after sampling') +
  xlab('Data') +
  ylab('Data points') 

```

Now, this is only with k=5. Let's explore k value with a defined set of ranges. Ideally, k has a value between 5 and 20. We shall run a for loop to get accuracy of each odd valued k to find the optimal one.

#Feature Selection 

Before doing the modeling section of our analysis, we decided to conduct a feature selection. A feature selection will help us to properly identify those features, or variables, that are most important to include in our models. In this manner, we can get the best model with the given data. To do identify these features, we decided to use three relevant feature selection models, these are: Spearman Correlation Plot, Lasso Regression and Bayesian Information Criterion (BIC) plot. Each feature selection model will interpret the data in their own manner and yield a different conclusion as to which feature is best to use. Therefore, if there is a feature that is indeed important, it should, in theory, be noticeable across each of these feature selection models.

## Feature Selection: Spearman Correlation Plot

Spearman Correlation Plots identify each correlation coefficient for ranked variables. A perk of using Spearmans Correlation Plot is that we can notice which relationships might result in multicollinearity, meaning that one independent variable being highly or absolutely correlated with another independent variable. Luckily for us, we do notice some substantial multicollinearity not only with one variable but with two in our first Spearman Correlation Plot. The variables `est_diameter_min` and `est_diameter_max` are perfectly positively correlated with each other, and perfectly negatively correlated with the variable `absolute_magnitude`. So, for the previously mentioned reasons, we will re-do our Spearman Correlation Plot without both of these variables. Thus, we are left with `relative_velocity`, `miss_distance` and `absolute_magnitude` as our independent variables. As for the interpretation with our dependent variable `hazardous` this second Spearman Correlation Plot reveals to us that `absolute_magnitude` has the highest degree of influence with a ***-0.37** coefficient, then `relative_velocity` with a **0.18** coefficient, and lastly `miss_distance` with virtualy no degree of influence **0.04**.

```{r, results='markup'}
library(corrplot)

neo_cor <- select(neo, c(3:6,9,10))
neo_cor$hazardous = as.numeric(as.logical(neo_cor$hazardous))

corrplot(cor(neo_cor, method ='spearman'), method = "number", title = "Spearman Correlation", outline = TRUE, mar = c(1,1,1,1))

neo_cor2 <- select(neo, c(4:6,9,10))
neo_cor2$hazardous = as.numeric(as.logical(neo_cor2$hazardous))

corrplot(cor(neo_cor2, method ='spearman'), method = "number", title = "Spearman Correlation", outline = TRUE, mar = c(1,1,1,1))

neo_cor3 <- select(neo, c(5:6,9,10))
neo_cor3$hazardous = as.numeric(as.logical(neo_cor3$hazardous))

corrplot(cor(neo_cor3, method ='spearman'), method = "number", title = "Spearman Correlation", outline = TRUE, mar = c(1,1,1,1))

library('ggplot2')
library('GGally')
ggpairs(neo_cor3)
```

## Feature Selection: Lasso Regression 

Next up in our feature selection models, we conducted a Lasso Regression. Lasso Regressions are elastic-net regularization methods that seek to add a penalty term for each independent coefficient in the model. Particularly, a Lasso Regression will add a L-1 penalty term, which is similar to a Manhattan distance, for varying degrees of lambda until each coefficient is reduced to zero. Ultimately, the logic behind a Lasso Regression is that the last coefficient to be converted to zero tends to be the most significant. Also, for this particular case since our dependent variable is categorical, we used binomial deviance as our measure to find the smallest degree of error by the log of lambda. As can be visualized in our first graph, the smallest degree of binomial deviance is when log lambda is equal to **7**, hence this represents a good starting point for our Lasso Regression. As can be noticed in our second graph, the coefficients start at the smalles point of error and eventually, as log lambda increases, all the coefficients turn to zero. However, `absolute_magnitude` by a landslide appeared to be the most significant coefficient as, unlike `miss_distance` and `relative_velocity`, it was the last to turn to zero. Which basically confirms our results in the previous Spearman Correlation Plot.

```{r, results='markup'}

library(glmnet)

newdata_lasso <- newdata

newdata_lasso$hazardous <- as.factor(newdata_lasso$hazardous)

y = newdata_lasso[,"hazardous"]

newdata_lasso_x <- scale(newdata_lasso[1:3], center = TRUE, scale = TRUE)
newdata_lasso_x <- data.frame(newdata_lasso)

x = model.matrix(hazardous ~ relative_velocity + absolute_magnitude + miss_distance, data=newdata_lasso)[,-1]

cv.lambda.lasso <- cv.glmnet(x=x, y=y, family = "binomial", alpha = 1)

plot(cv.lambda.lasso)   
title(main = 'Cross-Validation of Binomial Deviance vs. Log Lambda', line = 0.2)

cv.lambda.lasso

library(plotmo)
plot_glmnet(cv.lambda.lasso$glmnet.fit, 
     "lambda", label =TRUE, main = 'Lasso Regression')
```

## Feature Selection: Bayesian Information Criterion (BIC) plot

Last in the line-up of our feature selection models, we decided to conduct a BIC plot. A BIC plot shows us the lowest Bayesian Information Criterion point for each possible model. The logic behind the plot is that the lowest BIC possible represents the best possible the model. From our graph, we can notice that for the lowest BIC possible of **-87,000** the best model is that which includes all of the independent variables. And curiously enough, for each of the other lowest possible BIC levels, `absolute_magnitude` is included.

```{r, results='markup'}

newdata_glm <- newdata
newdata_glm$hazardous <- as.factor(newdata_glm$hazardous)

loadPkg("leaps")
reg.leaps <- regsubsets(hazardous~., data = newdata_glm, nbest = 1, method = "exhaustive")
plot(reg.leaps, scale = "bic", main = "BIC", las =1)

```

Thus, from our feature selections models, we have gathered that the best possible model is that which uses all of the independent variables, and that the most significant variable is `absolute_magnitude`, which recall represents the degree luminosity, calculated by its distance orbited to the sun, of the nearest earth objects.

# KNN
```{r, results='markup'}
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
set.seed(222)
for (kval in seq(1,18,2)) {
  nasa_pred <- knn(train = nasa_train, test = nasa_test, cl=nasa_train.labels, k=kval)
  cm = confusionMatrix(nasa_pred, as.factor(nasa_test.labels) ) # from caret library
  cmaccu = cm$overall['Accuracy']
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  ResultDf = rbind(ResultDf, cmt)
}
xkabledply(ResultDf, "Total Accuracy Summary:")

```

Now, with the accuracies defined for all possible k values, we shall draw a plot and see the accuracies.
```{r, results='markup'}
ggplot(data=ResultDf, aes(x=k, y=Total.Accuracy,group=1)) +
  geom_line(color="#aa0022", size=1.75) +
  geom_point(color="#aa0022", size=3.5) +
  ggtitle("K value against their accuracy in KNN model") +
  labs(x="K value", y="Accuracy of K value") +
  theme(axis.title.y = element_text(size=10, family="Trebuchet MS", color="#666666")) +
  theme(axis.text = element_text(size=14, family="Trebuchet MS")) +
  theme(plot.title = element_text(size=20, family="Trebuchet MS", face="bold", hjust=0, color="#666666"))


```
From the above, we see that when k=9, we get the good accuracy of 87.6%
Please note that despite having higher accuracies beyond k=9, we should keep in mind that having a higher k value leads to overfitting of the model and thus, we need to be mindful when choosing our k value.

Now, taking k=9, we should perform model evaulation.

## KNN model Evaluation
```{r, results='markup'}

knn.9 <- knn(train=nasa_train, test=nasa_test, cl=nasa_train.labels, k=9, prob=TRUE)
print(confusionMatrix(knn.9, as.factor(nasa_test.labels), mode = "everything"))

library(pROC)

print(roc(nasa_test.labels, attributes(knn.9)$prob))

plot(roc(nasa_test.labels, attributes(knn.9)$prob),
     print.thres = T,
     print.auc=T)


```
From the above, we see that the recall we obtain is 80% which is better than all of the above sampling techniques we have used above. In addition to this, we also see that the area under the curve is 0.706. Ideally, anything over 0.8 is considered good fit model. But area under the curve = 0.706 is considered an acceptable model. In conclusion, the KNN model with SMOTE sampling resulted in a good accuracy turnout. In addition to this, when k=11, we get the best accuracy in comparison with the rest.

#Logistic Regression 

## Interpretation of coefficients

The equation from the Logistic Regression model: 

logit(y(`hazardous`)) = 1.89e+01 + 1.89e+01* (`relative_velocity`) - 8.65e-01* (`absolute_magnitude`) - 1.50e-08* (`miss_distance`)

As hinted from our feature selection, all of the independent variable's coefficients are statistically significant. To make our interpretation of the coefficients even simpler, we decided that it would be best to describe them in terms of their odds, not log odds. Therefore, from our results we can observe that the odds of a neo being hazardous increases when its relative velocity increases, its absolute magnitude decreases and its miss distance decreases. 

```{r, results='markup'}

glm.nasa <- glm(formula = hazardous ~ ., data = newdata_glm, family = binomial)

summary(glm.nasa)

```

## Logistic Regression: Evaluation Metrics


* Confusion Matrix: With the addition of SMOTE to our dataframe we where able to improve our confusion matrix results to yield a higher recall score. From this particular model, the accuracy turned out to be **85.30%**, specificity (FPR) **91.16%** and Recall (TPR) **80.73%**. Therefore, the model did a good job at predicting actual true hazardous neo's, as well as an even better job at predicting actual not hazardous neo's.

* McFadden's value: The McFadden value, or pseudo r^2, was of **0.40**. So the model was able to interpret about 40% of the variance in the outcome of `hazardous`.


* Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC): The ROC curve appeared to be very steep indicating that it had a good True-Positive Rate (TPR) by False-Positive Rate (FPR), indicating that it performed substantially well. And lastly, the AUC was of **0.877** indicating that this was a good model to accept.

```{r, results='markup'}
set.seed(123)
#Confusion matrix
loadPkg("ModelMetrics")
xkabledply( confusionMatrix(actual= glm.nasa$y, predicted= glm.nasa$fitted.values), title = "Confusion matrix from Logit Model" )

#McFadden's value
loadPkg("pscl") 
model1r2 = pR2(glm.nasa)
model1r2

#Receiver-Operator-Characteristic (ROC) curve and Area-Under-Curve (AUC)
loadPkg("pROC")
prob1 = predict(glm.nasa, type ='response')
glm.nasa$prob = prob1
h1 = roc(hazardous ~ prob1, newdata_glm)
h1$auc
plot(h1, main = 'ROC: Logistic Regression Model')

glm.nasa$coefficients

```


## Logistic Regression: Visualization

From this graph we are able to identify each predicted probability, of our Logistic Regression model, by each value of `absolute_magnitude`. As observed, we notice that a good heuristic to have when it comes to hazardous or not-hazardous neo's is whether its `absolute_magnitude` is greater or less than 22. From our model, if the `absolute_magnitude` is greater than 22 then it predicts the neo to not be hazardous, and if the `absolute_magnitude` is less than 22 it predicts the neo to be hazardous. If the neo's `absolute_magnitude` is equal to 22 then it is a 50-50% chance of it being hazardous or not. This is a good heuristic to have in mind.

```{r, results='markup'}

glm_plot1 <- with(neo_cor3, data.frame(relative_velocity = mean(relative_velocity), miss_distance = mean(miss_distance), absolute_magnitude = rep(seq(from = 9, to = 34, length.out = 100), 2)))

glm_plot2 <- cbind(glm_plot1, predict(glm.nasa, newdata = glm_plot1, type = "link", se = TRUE))
glm_plot2 <- within(glm_plot2, {
    PredictedProb <- plogis(fit)
    LL <- plogis(fit - (1.96 * se.fit))
    UL <- plogis(fit + (1.96 * se.fit))
})

ggplot(glm_plot2, aes(x = absolute_magnitude, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
    ymax = UL), alpha = 0.2) + geom_line(size = 1, color = 'red') + labs(title = "Logistic model of predicted probability vs absolute_magnitude")

```

## Sample Predictions

* Prediction #1 (2022 AP7): We decided to compare our results with an recent and actual neo that is hazardous. Luckily for us, the model was able to predict with a **90.10%** predicted probability that the asteroid was indeed hazardous.

* Prediction #2 (2017 CM): We decided to compare our results with an recent and actual neo that was NOT hazardous. Luckily for us, the model was able to predict with a **36.80%** predicted probability that the asteroid was indeed NOT hazardous.

```{r, results='markup'}

sampleprediction1 = data.frame(relative_velocity = 17.38, absolute_magnitude = 17.1, miss_distance = 1.2417e+8)

sampleprediction2 = data.frame(relative_velocity = 21.80, absolute_magnitude = 18.02, miss_distance = 2.543e+8)

predict(glm.nasa, newdata = sampleprediction1, type = 'response')

predict(glm.nasa, newdata = sampleprediction2, type = 'response')

```

# Decision Trees

```{r}
install.packages("rpart", repos='http://cran.us.r-project.org') 
library(rpart)
```
```{r, fig.dim=c(8,8)}
set.seed(1)
newtrees = newdata
#neo <- read.csv("neo.csv")
neofit <- rpart(hazardous ~  absolute_magnitude + miss_distance + relative_velocity, data=newtrees, method="class", control = list(maxdepth = 4) )
printcp(neofit) # display the results 
plotcp(neofit) # visualize cross-validation results 
summary(neofit) # detailed summary of splits

# plot tree 
plot(neofit, uniform=TRUE, main="Classification Tree for NEO")
text(neofit, use.n=TRUE, all=TRUE, cex=0.9, col="red")
```
```{r}
post(neofit, file = "neoTree.ps", title = "Classification Tree for NEO")
```

```{r, include=T}
library(caret)
neo[,'hazardous'] <- as.factor(neo[,'hazardous']
cm = confusionMatrix( predict(neofit, type = "class"), reference = neo[, "hazardous"] )
xkabledply(cm$table, "confusion matrix")
print('Overall: ')
cm$overall
print('Class: ')
cm$byClass
```

```{r}
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:6) {
neofit2 <- rpart(hazardous ~  absolute_magnitude + miss_distance + relative_velocity, data=neo, method="class", control = list(maxdepth = deep) )
cm2 = confusionMatrix( predict(neofit2, type = "class"), reference = neo[, "hazardous"] )
 cmaccu = cm2$overall['Accuracy']
 cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL )
cmt = cbind( cmt, data.frame( t(cm$byClass) ) )
 confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
}
```

```{r, results="asis"}
xkabledply(confusionMatrixResultDf, title="NEO Classification Trees with Varying Max Depth")
```

```{r fancyplot}
#install.packages("rpart.plot", repos='http://cran.us.r-project.org')
library(rpart.plot)
#install.packages("rattle", repos='http://cran.us.r-project.org')
library(rattle)
rpart.plot(neofit)
fancyRpartPlot(neofit)
```

```{r}
install.packages("ISLR",repos='http://cran.us.r-project.org')
library(ISLR)
install.packages("tree",repos='http://cran.us.r-project.org')
library("tree")
treefit <- tree(log(miss_distance) ~ relative_velocity  + absolute_magnitude, data=newtrees)
summary(treefit)
```

```{r}
plot(treefit)
text(treefit,cex=0.75, col="blue")
```


```{r}
install.packages("caret", repos='http://cran.us.r-project.org')
library(caret)
loadPkg("caret")
confusionMatrixResultDf = data.frame( Depth=numeric(0), Accuracy= numeric(0), Sensitivity=numeric(0), Specificity=numeric(0), Pos.Pred.Value=numeric(0), Neg.Pred.Value=numeric(0), Precision=numeric(0), Recall=numeric(0), F1=numeric(0), Prevalence=numeric(0), Detection.Rate=numeric(0), Detection.Prevalence=numeric(0), Balanced.Accuracy=numeric(0), row.names = NULL )

for (deep in 2:6) {
neofit2 <- rpart(hazardous ~  absolute_magnitude + miss_distance + relative_velocity, data=newtrees, method="class", control = list(maxdepth = deep) )
cm2 = caret::confusionMatrix( predict(neofit2, type = "class"), reference = as.factor(newtrees[, "hazardous"] ))
 cmaccu = cm2$overall['Accuracy']
 cmt = data.frame(Depth=deep, Accuracy = cmaccu, row.names = NULL )
cmt = cbind( cmt, data.frame( t(cm$byClass) ) )
 confusionMatrixResultDf = rbind(confusionMatrixResultDf, cmt)
}
```

```{r, results="asis"}
xkabledply(confusionMatrixResultDf, title="NEO Classification Trees with Varying Max Depth")
```


## AUC for Decision 
```{r, results='markup'}
library(ROCR)
tree.preds <- predict(neofit2, as.data.frame(nasa_test),type = "prob")[,2]
library(pROC)
tree.roc <- roc(nasa_test.labels, tree.preds)
print(tree.roc)
plot(tree.roc)


```

# Sample Predictions

```{r, results='markup'}

sampleprediction1 = data.frame(relative_velocity = 17.38, absolute_magnitude = 17.1, miss_distance = 1.2417e+8)

sampleprediction2 = data.frame(relative_velocity = 21.80, absolute_magnitude = 18.02, miss_distance = 2.543e+8)

predict(glm.nasa, newdata = sampleprediction1, type = 'response')

predict(glm.nasa, newdata = sampleprediction2, type = 'response')

```

# Limitations

The `neo.csv` data set posed limitations in the variables, description and dictionary. Since the dataset was obtained from a web-based environment, kaggle, we had insufficient variables, a poor description of the dataset and no data dictionary provided. In terms of the variables provided, it would have been ideal to have one regarding the trajectory towards, or away, from earth, as well as other potentially hazardous, or not hazardous, variables. Nevertheless, we were restricted to just using the three variables that were quantifiable in order to characterize the potential of it being hazardous. In retrospect, just using these three variables might have been an over-simplification in our determination of a hazard. On the other hand, the description of the dataset is very limited given the constraint of using Kaggle. It would have been better if the dataset was obtained straight from a NASA website or API, however this was not possible. It would have been a huge perk to know how these variables are being used by NASA professionals, what characterizes an instance to be recorded multiple times, and why they initially attributed the boolean value for Hazardous. Finally, when it came to providing a data dictionary, the kaggle website did not have one. Thus, we had to do our own research as to what these variables meant, and what they were describing. Therefore, with insufficient variables, a poor description and no data dictionary it was difficult to infer, or fully understand, the root of a hazardous or not hazardous asteroid to fully answer our question.


# References:

- Kaggle Dataset (Kaggle):
Vani, S. (2022, June 17). NASA - Nearest Earth objects. Kaggle. Retrieved November 2, 2022, from https://www.kaggle.com/datasets/sameepvani/nasa-nearest-earth-objects 
- Center for Near Earth Objects Studies (CNEOS):
NASA. (n.d.). Neo basics. NASA. Retrieved November 2, 2022, from https://cneos.jpl.nasa.gov/about/neo_groups.html 
- NASA (NASA):
Bardan, R. (2022, October 11). NASA confirms Dart Mission Impact Changed Asteroid's motion in space. NASA. Retrieved November 2, 2022, from https://www.nasa.gov/press-release/nasa-confirms-dart-mission-impact-changed-asteroid-s-motion-in-space 
- Araujo, R. A. N., & Winter, O. C. (2014). Near-Earth asteroid binaries in close encounters with the Earth. Astronomy & Astrophysics, 566, A23.
- Peter O. K. Krehl (2008). History of Shock Waves, Explosions and Impact: A Chronological and Biographical Reference. Springer Science & Business Media. p. 404. ISBN 978-3-540-30421-0.
- Tonry, J. L. (2010). An early warning system for asteroid impact. Publications of the Astronomical Society of the Pacific, 123(899), 58.






